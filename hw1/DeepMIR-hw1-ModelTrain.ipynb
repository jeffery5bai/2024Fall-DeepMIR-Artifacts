{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emma/anaconda3/envs/bai_env310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: feature_extractor_cqt requires the libray 'nnAudio'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emma/anaconda3/envs/bai_env310/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from glob import glob\n",
    "import librosa\n",
    "from sklearn.metrics import precision_score, f1_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import AutoModel\n",
    "import warnings\n",
    "\n",
    "from model import CNNClassifier, ShortChunkCNN_Res\n",
    "\n",
    "# TODO: change the file path\n",
    "TRAIN_FILE_DIR = \"./hw1/slakh/train\"\n",
    "VALID_FILE_DIR = \"./hw1/slakh/validation\"\n",
    "TEST_FILE_DIR = \"./hw1/slakh/test\"\n",
    "TRAIN_LABEL_PATH = \"./hw1/slakh/train_labels.json\"\n",
    "VALID_LABEL_PATH = \"./hw1/slakh/validation_labels.json\"\n",
    "TEST_LABEL_PATH = \"./hw1/slakh/test_labels.json\"\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "LABELS = ['Piano', 'Percussion', 'Organ', 'Guitar', 'Bass', 'Strings', 'Voice', 'Wind Instruments', 'Synth']\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE: GPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-params\n",
    "EPOCHS = 30\n",
    "PATIENCE = 10\n",
    "BATCH_SIZE = 32  # 64\n",
    "LR = 1e-3  # 1e-5\n",
    "THRESHOLD = 0.7543 # current best\n",
    "\n",
    "# for model\n",
    "N_CHANNELS = 128  # 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: feature_extractor_cqt requires the libray 'nnAudio'\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained pre-processor and model\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-330M\",trust_remote_code=True)\n",
    "MERT_model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-330M\", trust_remote_code=True)\n",
    "\n",
    "# Freeze the pretrained model's parameters\n",
    "for param in MERT_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, wav_directory: str, label_directory: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            directory (string): Path to the directory with all the .npy files.\n",
    "        \"\"\"\n",
    "        self.directory = wav_directory\n",
    "        self.files = os.listdir(wav_directory)  # List of all .npy files in the directory\n",
    "        with open(label_directory, \"r\") as f:\n",
    "            self.labels = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.directory, self.files[idx])\n",
    "        audio_wave = np.load(file_path)\n",
    "        label = np.array(self.labels[self.files[idx]], dtype=np.float32)\n",
    "        return audio_wave, label\n",
    "\n",
    "\n",
    "train_dataset = AudioDataset(TRAIN_FILE_DIR, TRAIN_LABEL_PATH)\n",
    "valid_dataset = AudioDataset(VALID_FILE_DIR, VALID_LABEL_PATH)\n",
    "test_dataset = AudioDataset(TEST_FILE_DIR, TEST_LABEL_PATH)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, path, verbose = False):\n",
    "    postfix = datetime.now().strftime(\"%m%d-%H-%M\")\n",
    "    if path is None:\n",
    "        path = f\"DL_model_{postfix}.pt\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    if verbose:\n",
    "        print(f\"model successfully saved to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DL model (ShortChunkCNN_Res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        train_dataloader: DataLoader, \n",
    "        valid_dataloader: DataLoader,\n",
    "        MERT_model: AutoModel = MERT_model,\n",
    "        processor: Wav2Vec2FeatureExtractor = processor,\n",
    "        threshold: float = THRESHOLD,\n",
    "        model_path: Optional[str] = None,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "\n",
    "    # Training and Validation Record\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "\n",
    "    # Early Stopping\n",
    "    best_loss = np.Inf\n",
    "    best_score = 0\n",
    "    cnt = 0\n",
    "\n",
    "    # Model\n",
    "    model = ShortChunkCNN_Res(n_channels=N_CHANNELS)\n",
    "\n",
    "    # Optimizer\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Training Start!!!\n",
    "    print(\"Training Start!!!\")\n",
    "    for epoch in tqdm(range(EPOCHS), disable=(not verbose)):\n",
    "        MERT_model = MERT_model.to(DEVICE)\n",
    "        model = model.to(DEVICE)\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "        train_true = torch.tensor([])\n",
    "        train_pred_p = torch.tensor([])\n",
    "\n",
    "        # Training with batches\n",
    "        for i, (train_wavs, train_label) in enumerate(train_dataloader):\n",
    "            train_wavs = train_wavs.cpu().numpy()\n",
    "            inputs = processor(train_wavs, sampling_rate=24000, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            train_label = train_label.to(DEVICE)\n",
    "\n",
    "            # pre-trained model\n",
    "            outputs = MERT_model(**inputs)\n",
    "            pretrained_output = outputs.last_hidden_state # [batch_size, 374 time, 1024 feature_dim]\n",
    "\n",
    "            # Trainable classifier\n",
    "            optimizer.zero_grad()\n",
    "            output = model(pretrained_output)\n",
    "            loss = loss_fn(output, train_label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate Score\n",
    "            train_label = train_label.cpu()\n",
    "            output = output.cpu()\n",
    "            train_true = torch.cat([train_true, train_label])\n",
    "            train_pred_p = torch.cat([train_pred_p, output])\n",
    "\n",
    "            # Delete Var\n",
    "            del train_wavs, train_label, inputs, outputs, pretrained_output, output\n",
    "            gc.collect()\n",
    "        \n",
    "        train_pred = (train_pred_p > threshold).float()\n",
    "        train_score = f1_score(train_true, train_pred, average=\"macro\")\n",
    "        train_loss_list.append(total_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_total_loss = 0\n",
    "        valid_true = torch.tensor([])\n",
    "        valid_pred_p = torch.tensor([])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for j, (valid_wavs, valid_label) in enumerate(valid_dataloader):\n",
    "                valid_wavs = valid_wavs.cpu().numpy()\n",
    "                inputs = processor(valid_wavs, sampling_rate=24000, return_tensors=\"pt\")\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                valid_label = valid_label.to(DEVICE)\n",
    "\n",
    "                # pre-trained model      \n",
    "                outputs = MERT_model(**inputs)\n",
    "                pretrained_output = outputs.last_hidden_state # [batch_size, 374 time, 1024 feature_dim]\n",
    "\n",
    "                output = model(pretrained_output)\n",
    "                loss = loss_fn(output, valid_label)\n",
    "                valid_total_loss += loss.item()\n",
    "\n",
    "                # Calculate Score\n",
    "                valid_label = valid_label.cpu()\n",
    "                output = output.cpu()\n",
    "                valid_true = torch.cat([valid_true, valid_label])\n",
    "                valid_pred_p = torch.cat([valid_pred_p, output])\n",
    "\n",
    "                # Delete Var\n",
    "                # del valid_wavs, valid_label, inputs, outputs, output, all_layer_hidden_states, time_reduced_hidden_states\n",
    "                del valid_wavs, valid_label, inputs, output\n",
    "                gc.collect()\n",
    "\n",
    "        valid_pred = (valid_pred_p > threshold).float()\n",
    "        valid_score = f1_score(valid_true, valid_pred, average=\"macro\")\n",
    "        valid_loss_list.append(valid_total_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: train loss: {total_loss:.4f}, train score: {train_score:.4f} || valid loss: {valid_total_loss:.4f}, valid score: {valid_score:.4f}. Threshold: {threshold}\")\n",
    "\n",
    "        # Delete Var\n",
    "        del train_true, train_pred, train_pred_p, valid_true, valid_pred, valid_pred_p\n",
    "        gc.collect()\n",
    "\n",
    "        # for early stopping\n",
    "        if valid_score <= best_score:    #valid_total_loss >= best_loss:\n",
    "            cnt += 1\n",
    "            if cnt >= PATIENCE:\n",
    "                # print(f\"Early Stopping at epoch: {epoch+1}, the best valid loss = {best_loss:.4f}\")\n",
    "                print(f\"Early Stopping at epoch: {epoch+1}, the best scores = {best_score:.4f}\")\n",
    "                break\n",
    "        else:\n",
    "            # best_loss = valid_total_loss\n",
    "            best_score = valid_score\n",
    "            cnt = 0\n",
    "            save_checkpoint(model, model_path)\n",
    "    \n",
    "    print(f\"Training complete!\")\n",
    "    best_model = ShortChunkCNN_Res(n_channels=N_CHANNELS)\n",
    "    best_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return best_model, train_loss_list, valid_loss_list\n",
    "\n",
    "\n",
    "def test(\n",
    "        test_dataloader: DataLoader, \n",
    "        model: ShortChunkCNN_Res,\n",
    "        processor: Wav2Vec2FeatureExtractor = processor,\n",
    "        MERT_model: AutoModel = MERT_model,\n",
    "        threshold: float = THRESHOLD,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "\n",
    "    # Test start\n",
    "    MERT_model = MERT_model.to(DEVICE)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    loss_fn = nn.BCELoss()\n",
    "    test_total_loss = 0\n",
    "    test_true = torch.tensor([])\n",
    "    test_pred_p = torch.tensor([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_wavs, test_label in tqdm(test_dataloader, disable=(not verbose)):\n",
    "            test_wavs = test_wavs.cpu().numpy()\n",
    "            inputs = processor(test_wavs, sampling_rate=24000, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            test_label = test_label.to(DEVICE)\n",
    "\n",
    "            # pre-trained model      \n",
    "            outputs = MERT_model(**inputs)\n",
    "            pretrained_output = outputs.last_hidden_state # [batch_size, time, 1024 feature_dim]\n",
    "\n",
    "            output = model(pretrained_output)\n",
    "            loss = loss_fn(output, test_label)\n",
    "            test_total_loss += loss.item()\n",
    "\n",
    "            # Calculate Score\n",
    "            test_label = test_label.cpu()\n",
    "            output = output.cpu()\n",
    "            test_true = torch.cat([test_true, test_label])\n",
    "            test_pred_p = torch.cat([test_pred_p, output])\n",
    "\n",
    "            # Delete Var\n",
    "            del test_wavs, test_label, inputs, outputs, pretrained_output, output\n",
    "            gc.collect()\n",
    "\n",
    "    test_pred = (test_pred_p > threshold).float()\n",
    "    test_score = precision_score(test_true, test_pred, average=\"macro\")\n",
    "    test_score_f1 = f1_score(test_true, test_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"Macro Precision: {test_score:.4f}\")\n",
    "    print(f\"Macro F1-score: {test_score_f1:.4f}\")\n",
    "    if verbose:\n",
    "        report = classification_report(test_true, test_pred, target_names=LABELS)\n",
    "        print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    return test_true, test_pred_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [09:01<4:21:38, 541.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 229.6959, train score: 0.3252 || valid loss: 52.2525, valid score: 0.3816. Threshold: 0.7543\n",
      "Epoch 2: train loss: 206.0148, train score: 0.3925 || valid loss: 50.3212, valid score: 0.4344. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [18:01<4:12:24, 540.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss: 189.4072, train score: 0.4620 || valid loss: 52.9554, valid score: 0.4928. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [36:05<3:54:34, 541.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss: 173.1609, train score: 0.5460 || valid loss: 87.2296, valid score: 0.4624. Threshold: 0.7543\n",
      "Epoch 5: train loss: 155.0965, train score: 0.6243 || valid loss: 52.8863, valid score: 0.5505. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [54:09<3:36:43, 541.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train loss: 133.4534, train score: 0.7077 || valid loss: 61.2099, valid score: 0.5437. Threshold: 0.7543\n",
      "Epoch 7: train loss: 108.6969, train score: 0.7813 || valid loss: 60.6404, valid score: 0.5890. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [1:03:10<3:27:33, 541.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train loss: 86.1249, train score: 0.8418 || valid loss: 68.5253, valid score: 0.5963. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [1:12:12<3:18:36, 541.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train loss: 65.6533, train score: 0.8845 || valid loss: 78.1392, valid score: 0.5970. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [1:21:13<3:09:32, 541.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 52.7591, train score: 0.9109 || valid loss: 106.6895, valid score: 0.6073. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [1:39:18<2:51:36, 541.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train loss: 42.7240, train score: 0.9309 || valid loss: 99.7268, valid score: 0.5822. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [1:48:19<2:42:31, 541.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train loss: 37.5315, train score: 0.9417 || valid loss: 106.7952, valid score: 0.5855. Threshold: 0.7543\n",
      "Epoch 13: train loss: 33.5149, train score: 0.9493 || valid loss: 167.3739, valid score: 0.6302. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [2:06:21<2:24:21, 541.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train loss: 30.1994, train score: 0.9546 || valid loss: 135.1796, valid score: 0.6265. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [2:15:23<2:15:20, 541.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train loss: 26.3084, train score: 0.9620 || valid loss: 127.6240, valid score: 0.5916. Threshold: 0.7543\n",
      "Epoch 16: train loss: 24.1216, train score: 0.9645 || valid loss: 125.7990, valid score: 0.6387. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [2:33:25<1:57:15, 541.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train loss: 23.1786, train score: 0.9661 || valid loss: 146.8373, valid score: 0.6151. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [2:42:26<1:48:14, 541.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: train loss: 22.4957, train score: 0.9679 || valid loss: 163.7165, valid score: 0.6163. Threshold: 0.7543\n",
      "Epoch 19: train loss: 20.1240, train score: 0.9720 || valid loss: 131.0082, valid score: 0.6388. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [3:00:29<1:30:12, 541.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: train loss: 19.0185, train score: 0.9733 || valid loss: 211.1457, valid score: 0.6373. Threshold: 0.7543\n",
      "Epoch 21: train loss: 18.2295, train score: 0.9742 || valid loss: 145.0899, valid score: 0.6436. Threshold: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [3:18:31<1:12:08, 541.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: train loss: 17.4674, train score: 0.9755 || valid loss: 151.3711, valid score: 0.6432. Threshold: 0.7543\n"
     ]
    }
   ],
   "source": [
    "model_path = \"DL_model_f1.pt\"\n",
    "model, train_loss, valid_loss = train(train_dataloader, valid_dataloader, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [01:38<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.6771\n",
      "Macro F1-score: 0.6496\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           Piano       0.90      0.97      0.93      3237\n",
      "      Percussion       0.44      0.23      0.30       383\n",
      "           Organ       0.47      0.48      0.47       671\n",
      "          Guitar       0.92      0.84      0.88      3194\n",
      "            Bass       0.96      0.99      0.97      3471\n",
      "         Strings       0.63      0.91      0.74      1930\n",
      "           Voice       0.66      0.33      0.44       939\n",
      "Wind Instruments       0.56      0.67      0.61      1599\n",
      "           Synth       0.57      0.45      0.50      1074\n",
      "\n",
      "       micro avg       0.78      0.80      0.79     16498\n",
      "       macro avg       0.68      0.65      0.65     16498\n",
      "    weighted avg       0.79      0.80      0.79     16498\n",
      "     samples avg       0.78      0.80      0.78     16498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"DL_model_f1.pt\"\n",
    "model = ShortChunkCNN_Res(n_channels=N_CHANNELS)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "y_true, y_pred_p = test(valid_dataloader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.9541, best score: 0.7207\n",
      "\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           Piano       0.90      0.95      0.92      3237\n",
      "      Percussion       0.56      0.18      0.27       383\n",
      "           Organ       0.54      0.39      0.45       671\n",
      "          Guitar       0.94      0.77      0.84      3194\n",
      "            Bass       0.96      0.98      0.97      3471\n",
      "         Strings       0.66      0.88      0.76      1930\n",
      "           Voice       0.72      0.25      0.37       939\n",
      "Wind Instruments       0.58      0.57      0.58      1599\n",
      "           Synth       0.63      0.38      0.47      1074\n",
      "\n",
      "       micro avg       0.82      0.76      0.79     16498\n",
      "       macro avg       0.72      0.60      0.63     16498\n",
      "    weighted avg       0.81      0.76      0.77     16498\n",
      "     samples avg       0.81      0.76      0.77     16498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search best threshold on valid set\n",
    "thresholds = np.logspace(-1, 0, 50)\n",
    "best_score = 0\n",
    "best_threshold = None\n",
    "for t in thresholds:\n",
    "    y_pred = (y_pred_p > t).float()\n",
    "    test_score = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    if test_score > best_score:\n",
    "        best_threshold = t\n",
    "        best_score = test_score\n",
    "\n",
    "y_pred = (y_pred_p > best_threshold).float()\n",
    "print(f\"Threshold: {best_threshold:.4f}, best score: {best_score:.4f}\")\n",
    "report = classification_report(y_true, y_pred, target_names=LABELS)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:58<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.7178\n",
      "Macro F1-score: 0.6202\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           Piano       0.88      0.96      0.92      1889\n",
      "      Percussion       0.59      0.16      0.25       243\n",
      "           Organ       0.56      0.31      0.40       461\n",
      "          Guitar       0.93      0.80      0.86      1943\n",
      "            Bass       0.96      0.99      0.97      2076\n",
      "         Strings       0.72      0.87      0.79      1235\n",
      "           Voice       0.69      0.30      0.42       485\n",
      "Wind Instruments       0.55      0.55      0.55       889\n",
      "           Synth       0.58      0.34      0.43       647\n",
      "\n",
      "       micro avg       0.82      0.76      0.79      9868\n",
      "       macro avg       0.72      0.59      0.62      9868\n",
      "    weighted avg       0.81      0.76      0.77      9868\n",
      "     samples avg       0.81      0.76      0.77      9868\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"DL_model_f1.pt\"\n",
    "model = ShortChunkCNN_Res(n_channels=N_CHANNELS)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_true, test_pred_p = test(test_dataloader, model, threshold=0.9541)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   1299 MiB |   5925 MiB | 540431 GiB | 540430 GiB |\n",
      "|       from large pool |   1293 MiB |   5909 MiB | 539032 GiB | 539031 GiB |\n",
      "|       from small pool |      5 MiB |     24 MiB |   1399 GiB |   1399 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   1299 MiB |   5925 MiB | 540431 GiB | 540430 GiB |\n",
      "|       from large pool |   1293 MiB |   5909 MiB | 539032 GiB | 539031 GiB |\n",
      "|       from small pool |      5 MiB |     24 MiB |   1399 GiB |   1399 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   1298 MiB |   5915 MiB | 540372 GiB | 540371 GiB |\n",
      "|       from large pool |   1293 MiB |   5899 MiB | 538974 GiB | 538972 GiB |\n",
      "|       from small pool |      5 MiB |     24 MiB |   1398 GiB |   1398 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   2766 MiB |   9506 MiB |  13314 MiB |  10548 MiB |\n",
      "|       from large pool |   2760 MiB |   9476 MiB |  13284 MiB |  10524 MiB |\n",
      "|       from small pool |      6 MiB |     30 MiB |     30 MiB |     24 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   1466 MiB |   6521 MiB | 512881 GiB | 512880 GiB |\n",
      "|       from large pool |   1466 MiB |   6518 MiB | 511454 GiB | 511453 GiB |\n",
      "|       from small pool |      0 MiB |      7 MiB |   1426 GiB |   1426 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     569    |    1045    |    9459 K  |    9459 K  |\n",
      "|       from large pool |     170    |     248    |    4958 K  |    4958 K  |\n",
      "|       from small pool |     399    |     797    |    4501 K  |    4500 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     569    |    1045    |    9459 K  |    9459 K  |\n",
      "|       from large pool |     170    |     248    |    4958 K  |    4958 K  |\n",
      "|       from small pool |     399    |     797    |    4501 K  |    4500 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      75    |      92    |      94    |      19    |\n",
      "|       from large pool |      72    |      77    |      79    |       7    |\n",
      "|       from small pool |       3    |      15    |      15    |      12    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       9    |      52    |    4193 K  |    4193 K  |\n",
      "|       from large pool |       6    |      20    |    2158 K  |    2158 K  |\n",
      "|       from small pool |       3    |      37    |    2034 K  |    2034 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(\n",
    "        flac_file_path: str,\n",
    "        model: ShortChunkCNN_Res,\n",
    "        processor: Wav2Vec2FeatureExtractor = processor,\n",
    "        MERT_model: AutoModel = MERT_model,\n",
    "        threshold: float = THRESHOLD,\n",
    "        save_file: bool = True,\n",
    "    ):\n",
    "    name = flac_file_path.split('/')[-1].split('.')[0]\n",
    "    a, sr = sf.read(flac_file_path)\n",
    "    n = librosa.resample(a, orig_sr=sr, target_sr=24000)\n",
    "    n = n[:-(n.shape[0]%120000)]  # remove trailing\n",
    "    n = n.reshape(((n.shape[0]//120000), 120000))  # reshape into 5 second\n",
    "    inputs = processor(n, sampling_rate=24000, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    MERT_model = MERT_model.to(DEVICE)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # pre-trained model\n",
    "    with torch.no_grad():\n",
    "        outputs = MERT_model(**inputs)\n",
    "        pretrained_output = outputs.last_hidden_state # [batch_size, time, 1024 feature_dim]\n",
    "        output = model(pretrained_output)\n",
    "\n",
    "    output = output.cpu()\n",
    "    output = (output > threshold).float()\n",
    "    output = output.numpy().T\n",
    "\n",
    "    if save_file:\n",
    "        np.save(f\"./hw1/test_track/{name}.npy\", output)\n",
    "        print(f\"File {name}.npy successfully saved. dim={output.shape}\")\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Track01937.npy successfully saved. dim=(9, 40)\n",
      "File Track01876.npy successfully saved. dim=(9, 51)\n",
      "File Track02100.npy successfully saved. dim=(9, 45)\n",
      "File Track02078.npy successfully saved. dim=(9, 43)\n",
      "File Track02024.npy successfully saved. dim=(9, 49)\n"
     ]
    }
   ],
   "source": [
    "model_path = \"DL_model_f1.pt\"\n",
    "model = ShortChunkCNN_Res(n_channels=N_CHANNELS)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "audio_path_list = glob(os.path.join(\"./hw1/test_track\", \"*.flac\"))\n",
    "for file in audio_path_list:\n",
    "    o = get_prediction(file, model=model, threshold=0.9541)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bai_env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
