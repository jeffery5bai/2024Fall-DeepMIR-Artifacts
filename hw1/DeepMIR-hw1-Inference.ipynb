{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emma/anaconda3/envs/bai_env310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: feature_extractor_cqt requires the libray 'nnAudio'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emma/anaconda3/envs/bai_env310/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from glob import glob\n",
    "import librosa\n",
    "from sklearn.metrics import precision_score, f1_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import AutoModel\n",
    "import warnings\n",
    "\n",
    "from model import ShortChunkCNN_Res\n",
    "\n",
    "# TODO: change the file path\n",
    "TEST_FILE_DIR = \"./hw1/slakh/test\"\n",
    "TEST_LABEL_PATH = \"./hw1/slakh/test_labels.json\"\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "LABELS = ['Piano', 'Percussion', 'Organ', 'Guitar', 'Bass', 'Strings', 'Voice', 'Wind Instruments', 'Synth']\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE: GPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-params\n",
    "EPOCHS = 30\n",
    "PATIENCE = 10\n",
    "BATCH_SIZE = 32  # 64\n",
    "LR = 1e-3  # 1e-5\n",
    "THRESHOLD = 0.9541 # current best\n",
    "\n",
    "# for model\n",
    "N_CHANNELS = 128  # 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: feature_extractor_cqt requires the libray 'nnAudio'\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained pre-processor and model\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-330M\",trust_remote_code=True)\n",
    "MERT_model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-330M\", trust_remote_code=True)\n",
    "\n",
    "# Freeze the pretrained model's parameters\n",
    "for param in MERT_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, wav_directory: str, label_directory: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            directory (string): Path to the directory with all the .npy files.\n",
    "        \"\"\"\n",
    "        self.directory = wav_directory\n",
    "        self.files = os.listdir(wav_directory)  # List of all .npy files in the directory\n",
    "        with open(label_directory, \"r\") as f:\n",
    "            self.labels = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.directory, self.files[idx])\n",
    "        audio_wave = np.load(file_path)\n",
    "        label = np.array(self.labels[self.files[idx]], dtype=np.float32)\n",
    "        return audio_wave, label\n",
    "\n",
    "\n",
    "test_dataset = AudioDataset(TEST_FILE_DIR, TEST_LABEL_PATH)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test DL model (ShortChunkCNN_Res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "        test_dataloader: DataLoader, \n",
    "        model: ShortChunkCNN_Res,\n",
    "        processor: Wav2Vec2FeatureExtractor = processor,\n",
    "        MERT_model: AutoModel = MERT_model,\n",
    "        threshold: float = THRESHOLD,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "\n",
    "    # Test start\n",
    "    MERT_model = MERT_model.to(DEVICE)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    loss_fn = nn.BCELoss()\n",
    "    test_total_loss = 0\n",
    "    test_true = torch.tensor([])\n",
    "    test_pred_p = torch.tensor([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_wavs, test_label in tqdm(test_dataloader, disable=(not verbose)):\n",
    "            test_wavs = test_wavs.cpu().numpy()\n",
    "            inputs = processor(test_wavs, sampling_rate=24000, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            test_label = test_label.to(DEVICE)\n",
    "\n",
    "            # pre-trained model      \n",
    "            outputs = MERT_model(**inputs)\n",
    "            pretrained_output = outputs.last_hidden_state # [batch_size, time, 1024 feature_dim]\n",
    "\n",
    "            output = model(pretrained_output)\n",
    "            loss = loss_fn(output, test_label)\n",
    "            test_total_loss += loss.item()\n",
    "\n",
    "            # Calculate Score\n",
    "            test_label = test_label.cpu()\n",
    "            output = output.cpu()\n",
    "            test_true = torch.cat([test_true, test_label])\n",
    "            test_pred_p = torch.cat([test_pred_p, output])\n",
    "\n",
    "            # Delete Var\n",
    "            del test_wavs, test_label, inputs, outputs, pretrained_output, output\n",
    "            gc.collect()\n",
    "\n",
    "    test_pred = (test_pred_p > threshold).float()\n",
    "    test_score = precision_score(test_true, test_pred, average=\"macro\")\n",
    "    test_score_f1 = f1_score(test_true, test_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"Macro Precision: {test_score:.4f}\")\n",
    "    print(f\"Macro F1-score: {test_score_f1:.4f}\")\n",
    "    if verbose:\n",
    "        report = classification_report(test_true, test_pred, target_names=LABELS)\n",
    "        print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    return test_true, test_pred_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:58<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.7178\n",
      "Macro F1-score: 0.6202\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           Piano       0.88      0.96      0.92      1889\n",
      "      Percussion       0.59      0.16      0.25       243\n",
      "           Organ       0.56      0.31      0.40       461\n",
      "          Guitar       0.93      0.80      0.86      1943\n",
      "            Bass       0.96      0.99      0.97      2076\n",
      "         Strings       0.72      0.87      0.79      1235\n",
      "           Voice       0.69      0.30      0.42       485\n",
      "Wind Instruments       0.55      0.55      0.55       889\n",
      "           Synth       0.58      0.34      0.43       647\n",
      "\n",
      "       micro avg       0.82      0.76      0.79      9868\n",
      "       macro avg       0.72      0.59      0.62      9868\n",
      "    weighted avg       0.81      0.76      0.77      9868\n",
      "     samples avg       0.81      0.76      0.77      9868\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"DL_model_f1.pt\"\n",
    "model = ShortChunkCNN_Res(n_channels=N_CHANNELS)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_true, test_pred_p = test(test_dataloader, model, threshold=THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(\n",
    "        flac_file_path: str,\n",
    "        model: ShortChunkCNN_Res,\n",
    "        processor: Wav2Vec2FeatureExtractor = processor,\n",
    "        MERT_model: AutoModel = MERT_model,\n",
    "        threshold: float = THRESHOLD,\n",
    "        save_file: bool = True,\n",
    "    ):\n",
    "    name = flac_file_path.split('/')[-1].split('.')[0]\n",
    "    a, sr = sf.read(flac_file_path)\n",
    "    n = librosa.resample(a, orig_sr=sr, target_sr=24000)\n",
    "    n = n[:-(n.shape[0]%120000)]  # remove trailing\n",
    "    n = n.reshape(((n.shape[0]//120000), 120000))  # reshape into 5 second\n",
    "    inputs = processor(n, sampling_rate=24000, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    MERT_model = MERT_model.to(DEVICE)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # pre-trained model\n",
    "    with torch.no_grad():\n",
    "        outputs = MERT_model(**inputs)\n",
    "        pretrained_output = outputs.last_hidden_state # [batch_size, time, 1024 feature_dim]\n",
    "        output = model(pretrained_output)\n",
    "\n",
    "    output = output.cpu()\n",
    "    output = (output > threshold).float()\n",
    "    output = output.numpy().T\n",
    "\n",
    "    if save_file:\n",
    "        np.save(f\"./hw1/test_track/{name}.npy\", output)\n",
    "        print(f\"File {name}.npy successfully saved. dim={output.shape}\")\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Track01937.npy successfully saved. dim=(9, 40)\n",
      "File Track01876.npy successfully saved. dim=(9, 51)\n",
      "File Track02100.npy successfully saved. dim=(9, 45)\n",
      "File Track02078.npy successfully saved. dim=(9, 43)\n",
      "File Track02024.npy successfully saved. dim=(9, 49)\n"
     ]
    }
   ],
   "source": [
    "model_path = \"DL_model_f1.pt\"\n",
    "model = ShortChunkCNN_Res(n_channels=N_CHANNELS)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# TODO: change the file path\n",
    "audio_path_list = glob(os.path.join(\"./hw1/test_track\", \"*.flac\"))\n",
    "for file in audio_path_list:\n",
    "    o = get_prediction(file, model=model, threshold=THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bai_env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
